a1.sources=r1
a1.sinks=k1
a1.channels=c1

# 源类型为 TAILDIR，用于监控文件夹中的新文件
a1.sources.r1.type=TAILDIR
# 记录源 r1 的读取位置的文件路径
a1.sources.r1.positionFile=/root/1.json
# 定义文件组 f1
a1.sources.r1.filegroups=f1 f2 f3 f4 f5 f7
# 文件组 f1 包含匹配正则表达式
a1.sources.r1.filegroups.f1=公司简介*
a1.sources.r1.filegroups.f2=公司高管*
a1.sources.r1.filegroups.f3=所属指数*
a1.sources.r1.filegroups.f4=所属概念板块*
a1.sources.r1.filegroups.f5=所属系*
a1.sources.r1.filegroups.f6=所属行业板块*
a1.sources.r1.filegroups.f7=相关证券*
# 设置不同目录采集区分的文件头
# a1.sources.r1.headers.f1.headerKey1=公司简介
# a1.sources.r1.headers.f2.headerKey1=公司高管
# a1.sources.r1.headers.f3.headerKey1=所属指数
# a1.sources.r1.headers.f4.headerKey1=所属概念板块
# a1.sources.r1.headers.f5.headerKey1=所属系
# a1.sources.r1.headers.f6.headerKey1=所属行业板块
# a1.sources.r1.headers.f7.headerKey1=相关证券
a1.sources.r1.headers.f1.headerKey1=t1
a1.sources.r1.headers.f2.headerKey1=f2
a1.sources.r1.headers.f3.headerKey1=f3
a1.sources.r1.headers.f4.headerKey1=f4
a1.sources.r1.headers.f5.headerKey1=f5
a1.sources.r1.headers.f6.headerKey1=f6
a1.sources.r1.headers.f6.headerKey1=f6

#配置数据输出到HDFS的sink组件k1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop102:8020/flume/%{headerKey1}/%H
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = logs-
#是否按照时间滚
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位
a1.sinks.k1.hdfs.roundUnit = hour
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
a1.sinks.k1.hdfs.batchSize = 100
#设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 60000
#设置每个文件的滚动大小
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a1.sinks.k1.hdfs.rollCount = 100000

a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100

a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1